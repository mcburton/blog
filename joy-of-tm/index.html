<html>
	<head>
		<title>
			Topic Modeling for JDH
		</title>
		<meta charset="UTF-8">
		<style type="text/css">
@import url('https://themes.googleusercontent.com/fonts/css?kit=lhDjYqiy3mZ0x6ROQEUoUw');ol{margin:0;padding:0}.c10{vertical-align:top;width:117pt;border-style:solid;background-color:#c9daf8;border-color:#000000;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c7{vertical-align:top;width:90pt;border-style:solid;border-color:#000000;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c23{vertical-align:top;width:75.8pt;border-style:solid;border-color:#000000;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c19{vertical-align:top;width:105.8pt;border-style:solid;border-color:#000000;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c26{vertical-align:top;width:102pt;border-style:solid;border-color:#000000;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c30{color:#525252;font-size:9pt;background-color:#ffffff;font-family:"Arial";text-decoration:underline}.c25{line-height:1.5;padding-top:10pt;padding-bottom:12pt}.c32{max-width:468pt;background-color:#ffffff;padding:72pt 72pt 72pt 72pt}.c4{line-height:1.5;padding-top:14pt;padding-bottom:4pt}.c13{text-indent:36pt;padding-bottom:10pt}.c35{height:1px;width:33%}.c0{font-size:10pt;font-style:italic}.c2{text-align:center;direction:ltr}.c34{margin-right:36pt;text-align:justify}.c8{color:inherit;text-decoration:inherit}.c11{color:#1155cc;text-decoration:underline}.c15{height:12pt}.c5{font-style:italic}.c33{padding-top:0pt}.c1{font-size:10pt}.c21{font-size:18pt}.c29{padding-top:11.2pt}.c16{height:0pt}.c17{margin-right:-1.5pt}.c18{color:#666666}.c6{margin-left:36pt}.c24{border-collapse:collapse}.c22{text-indent:36pt}.c12{font-weight:bold}.c3{direction:ltr}.c27{padding-top:10pt}.c28{background-color:#d9ead3}.c14{background-color:#b6d7a8}.c20{line-height:1.0}.c9{font-family:"Consolas"}.c31{padding-bottom:10pt}.title{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:36pt;font-family:"Georgia";font-weight:bold;padding-bottom:6pt}.subtitle{padding-top:1pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:24pt;font-family:"Georgia";padding-bottom:4pt}li{color:#000000;font-size:12pt;font-family:"Georgia"}p{color:#000000;font-size:12pt;margin:0;font-family:"Georgia"}h1{padding-top:0pt;line-height:1.15;text-align:left;color:#000000;font-size:24pt;font-family:"Georgia";font-weight:bold;padding-bottom:0pt}h2{padding-top:0pt;line-height:1.15;text-align:left;color:#000000;font-size:18pt;font-family:"Georgia";font-weight:bold;padding-bottom:0pt}h3{padding-top:0pt;line-height:1.15;text-align:left;color:#000000;font-size:14pt;font-family:"Georgia";font-weight:bold;padding-bottom:0pt}h4{padding-top:0pt;line-height:1.15;text-align:left;color:#000000;font-size:12pt;font-family:"Georgia";font-weight:bold;padding-bottom:0pt}h5{padding-top:0pt;line-height:1.15;text-align:left;color:#000000;font-size:9pt;font-family:"Georgia";font-weight:bold;padding-bottom:0pt}h6{padding-top:0pt;line-height:1.15;text-align:left;color:#000000;font-size:8pt;font-family:"Georgia";font-weight:bold;padding-bottom:0pt}
		</style>
	</head>
	<body class="c32">
		<h1 class="c3">
			<a name="h.ium6cpc0ixq1"></a><span>The Joy of Topic Modeling</span>
		</h1>
		<p class="c3 c33 subtitle">
			<a name="h.fodh06tl8l4o"></a><span class="c21">A bag of words by Matt Burton on the 21st of May 2013</span>
		</p>
		<p class="c3">
			<span>Topic modeling is a catchall term for a group of computational techniques that, at a very high level, find patterns of co-occurrence in data (broadly conceived). In many cases, but not always, the data in question are words. More specifically, the frequency of words in documents. In natural language processing this is often called a “bag-of-words” model. A bag-of-words model has the effect of simplifying the complex structure of natural language by ignoring</span> <span>syntax</span><span>&nbsp;and grammar and focusing on the frequency of words within documents. So instead of a properly ordered, grammatically correct</span> <span>sentence, the</span><span>&nbsp;bag-of-words approach slices and dices</span> <span>text</span><span>&nbsp;into a table of words and frequency counts.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>You might wonder, “</span><span class="c5">How can we find meaning without structure? Without order the meaning is lost!</span><span>” Yes, significant context is</span> <span>lost</span><span>&nbsp;by only counting words in documents. Such concerns are absolutely correct, but counting words is still quite effective.</span><sup><a href="#ftnt1" name="ftnt_ref1">[1]</a></sup><span>&nbsp;My purpose here is not to engage in a prolonged argument about the epistemic validity of</span> <span>topic modeling’s underlying</span> <span>assumptions</span><span>;</span><span>&nbsp;</span><span>I merely want to describe them because I don’t think they have been well articulated in other introductions to topic modeling</span><span>. It is my hope as scholars from the humanities and interpretive social sciences learn more about topic modeling, text mining, and natural language processing, that their knowledge of language and writing will</span> <span class="c5">inform</span> <span>the state-of-the-art of text and language models. &nbsp;</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>To understand and interpret topic models, it is important to have a solid understanding of how topic models work. Topic models have been described</span> <span>from</span><span>&nbsp;a variety of perspectives, ranging from the metaphorical, like Jocker’s LDA Buffet,</span><sup><a href="#ftnt2" name="ftnt_ref2">[2]</a></sup><span>&nbsp;to the rigorously mathematical, like Blei, Ng, and Jordan’s article introducing LDA in the Journal of Machine Research,</span><sup><a href="#ftnt3" name="ftnt_ref3">[3]</a></sup><span>&nbsp;to the pragmatic, like Brett’s introduction in the Journal of Digital Humanities.</span><sup><a href="#ftnt4" name="ftnt_ref4">[4]</a></sup><span>&nbsp;My goal is to describe topic modeling by complementing existing introductions to topic modeling and filling some important bits of information they have left out.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>The following treatise has three parts. First, a brief jaunt into what I mean when I say “model.” Second, a deeper discussion into what I mean by</span> <span class="c5">word</span><span>,</span> <span class="c5">document</span><span>, and</span> <span class="c5">topic</span><span>. Third,</span> <span>a non-mathy description of</span><span>&nbsp;topic models by tracing the evolving complexity of four generative language models. &nbsp;Not everything I cover here is directly related to topic modeling, but I think much of what I cover are assumptions and information generally left out of most topic modeling conversations. It is difficult to understand how topic modeling works if you don’t understand natural language processing concepts like tokenization and stemming. Additionally, I think the distinction between a topic model’s generative process, and the estimation of a topic model’s parameters is an important detail left out of most discussions on topic modeling. Scholars interested in topic modeling need to know this stuff, so I have done my best to assemble it all together in one, perhaps messy, place.</span>
		</p>
		<h3 class="c3 c25">
			<a name="h.6mqyark2kzau"></a><span>On Models and Reality</span>
		</h3>
		<p class="c3">
			<span>The topic models I discuss here are known as</span> <span class="c5">generative</span><span>&nbsp;topic models. Generative models try to represent, in computational abstraction, a process by which documents in a corpus</span> <span>could</span><span>&nbsp;be authored.</span> <span>It</span><span>&nbsp;</span><span>is</span><span>&nbsp;important to recognize such computational models are not claiming “</span><span class="c5">this is how these documents were actually authored,”</span><span>&nbsp;rather they are probabilistic approximations of the document creation process. In the</span> <span class="c5">Companion to Digital Humanities</span><span>,</span><sup><a href="#ftnt5" name="ftnt_ref5">[5]</a></sup><span>&nbsp;Willard McCarty explores what the term modeling means in a computational context.</span>
		</p>
		<p class="c6 c3 c27 c31">
			<span class="c1">Two effects of computing make the distinction between "idea" or other sort of mental construct on the one hand, and on the other "model" in the sense we require: first, the demand for computational tractability, i.e., for complete explicitness and absolute consistency; second, the manipulability that a computational representation provides.</span>
		</p>
		<p class="c3 c31">
			<span>These two effects of computation that McCarty points out are crucial for understanding how topic modeling works and</span> <span>the</span><span>&nbsp;kinds of knowledge it produces. First,</span> <span class="c5">computational tractability</span><span>&nbsp;might be, for someone not trained in computer science or programming, a somewhat alien concept. To help illustrate this tractability problem, I want to share a wonderful anecdote</span> <span>from</span><span>&nbsp;early pioneer of computational art, Frieder Nake. In</span> <span>the</span><span>&nbsp;wonderful documentary,</span> <span class="c5">Hello World! Processing,</span><sup class="c1"><a href="#ftnt6" name="ftnt_ref6">[6]</a></sup><span class="c5">&nbsp;</span><span>Nake tells the story of an interaction between another early computational artist Georg Nees and the painter Hans Drucker at a 1965 exhibition of Nees’s computational art.</span>
		</p>
		<p class="c6 c3 c31">
			<span class="c1">The leadings fine artist, the painter Hans Drucker, raised his hand and said, “young man” addressing Georg Nees, “all said very well, what you told us, but you know what, could you make your machine draw the way I draw?” and Nees pondered for a moment and said, “you know what, if you tell me how you draw I can make my machine do it.”</span>
		</p>
		<p class="c3">
			<span>Nake explains how Drucker and Nees were both correct, Drucker assumed Nees’s answer would be “no,” because there is no way a machine could replicate the richness and complexity of a human’s artistic talent. Nees, however, pointed out that a machine can do anything</span> <span class="c5">if you can describe how to do it.</span><span>&nbsp;</span><span>Computational tractability requires models to be expressed using the explicit and precise mathematical language of algorithms. The challenge is not that a computer cannot produce (or replicate) art, it is that art, as with many processes, defy reification into a formalized set of steps. Much of human action and understanding lives in what Michael Polanyi calls the</span> <span class="c5">tacit dimension</span><span>,</span><sup><a href="#ftnt7" name="ftnt_ref7">[7]</a></sup><span>&nbsp;which</span> <span>is</span><span>&nbsp;best articulated by Polanyi’s famous</span> <span>aphorism</span><span>&nbsp;“we know more than we can tell.” Indeed, we know more than we can tell computers.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>The second effect of computing as described by McCarty involves an understanding of representational manipulability. When we describe a model and make it computationally tractable, we make it material (in a manner of speaking) and subject to, and the arbiter of, mechanical/computational manipulation. There is a deep sense of movement and change associated with computation; when we make our models tractable, we articulate a series of steps, an algorithm, for the computer. I sometimes like to</span> <span>joking</span><span>ly</span><span>&nbsp;think of computation as</span> <span class="c5">math with motion.</span><span>&nbsp;But what is crucial to understand with respect to movement and manipulability, is we do not</span> <span class="c5">know</span><span>&nbsp;what will come out of a computational process</span> <span class="c5">until it occurs</span><span>. McCarty connects this sense of movement to emergent understanding and knowledge. Models are, in McCarty’s words, “</span><span class="c5">temporary states in a process of coming to know</span><span>&nbsp;rather than fixed structures of knowledge.”</span><sup><a href="#ftnt8" name="ftnt_ref8">[8]</a></sup>
		</p>
		<h3 class="c4 c3">
			<a name="h.1m9jil1we9k3"></a><span>A World from a Topic Model’s Perspective</span>
		</h3>
		<p class="c3">
			<span>First and foremost, it is important to understand the strange meaning</span><span>s</span><span>&nbsp;</span><span class="c5">word, document,</span> <span>and</span><span class="c5">&nbsp;topic</span><span>&nbsp;assume in the world of language modeling</span><span class="c5">.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>At the start of any text mining adventure, t</span><span>he natural sequences of words, the sentences and paragraphs of written documents</span> <span>are</span> <span>broken up via a process called</span> <span class="c5">tokenization.</span><span>&nbsp;Individual words</span><span>&nbsp;</span><span>become</span> <span class="c5">unigrams</span><span>&nbsp;or individually unique tokens. Tokens are not always equivalent to words because the tokenization process may count two or more words together as a single token, creating what are called</span> <span class="c5">bigrams</span><span>&nbsp;or</span> <span class="c5">ngrams.</span><span>&nbsp;For example</span><span>, the words</span><span>&nbsp;“digital humanities” could be a bigram or two individual unigrams, “digital” and “humanities.” Tokenization is more of an art than a science, it requires subjective decisions as well as domain understanding of the texts being processed.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>There are typically two additional pre-processing steps applied to tokenized text before we can partake in the joy of topic</span> <span>modeling</span><span>. The first involves the removal of</span> <span class="c5">stop words</span><span>&nbsp;and the second is</span> <span class="c5">stemming.</span><span>&nbsp;I should note, there are other flavors of preprocessing, such as parts-of-speech tagging and removal, but I won’t be covering them here.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Once the beautiful prosaic text has been sliced and diced, it contains tokens like “and,” “but,” or “or.” These</span> <span class="c5">stop words</span> <span>are a wrench in the gears of bag-of-words language modeling producing incomprehensible or low-value output. Stop words lose their meaning once they have been decontextualized from their positions in the sequential order of the original texts. Stop words lists are often part of text mining or natural language processing software packages, posted</span> <span>on the web</span><span>, or passed around from researcher to researcher. Alternatively, they might be generated for a specific corpus using techniques</span> <span>like</span> <span class="c5">term frequency-inverse document frequency</span><sup class="c5"><a href="#ftnt9" name="ftnt_ref9">[9]</a></sup><span>&nbsp;ranking</span><span>, a technique that ranks a word’s prevalence in individual documents against their prevalence across a corpus of documents. This has the effect of giving words common across all documents, like stop words, a low ranking enabling the possibility of creating a corpus or domain specific stop word list by selecting all words with a score less than some specified value. &nbsp; &nbsp;</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Stopwords illustrate a couple interesting, and sometimes problematic, assumptions in the pre-processing of texts. Consider the string of words “to be or not to be.” This famous sequence of words is pregnant with meaning and implications, but in the eyes of a textual pre-processor it is completely</span> <span>mangled</span><span>. When the phrase is tokenized and counted we end up with the following representation devoid of its original meaning: [“to”:2, “be”:2, “or”:1, “not”:1, “to”1]. Further, when we filter stop words, every word in that famous phrase is completely removed.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Once the stop words have been removed, there are still morphological problems with word tokens to be overcome. Basic tokenization and term frequency is going to count “model” and “models” as separate tokens. This can be a problem</span> <span>because</span><span>&nbsp;we</span> <span class="c5">want</span><span>&nbsp;these tokens to be counted together. Stemming is a process that</span> <span>trims</span><span>&nbsp;word tokens down to their morphological roots.</span> <span>D</span><span>ifferent algorithms stem more or less aggressively. A lightweight stemmer might remove pluralization or other suffixes, a more aggressive stemmer cuts words back to their lexicographical root. One very popular and aggressive algorithm</span> <span>used in fulltext search and information retrieval</span><span>, the Porter stemmer,</span><sup><a href="#ftnt10" name="ftnt_ref10">[10]</a></sup><span>&nbsp;trims words to incomprehen</span><span>sion;</span><span>&nbsp;"example" becomes "exampl" and "courage" becomes "courag." Such aggressive stemming is generally not very useful for topic modeling because the</span> <span>topics</span><span>&nbsp;become difficult to interpret</span> <span>because</span><span>&nbsp;word</span><span>’s</span> <span>morphological roots may have different</span><span>&nbsp;meanings</span><span>.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span class="c5">Documents,</span> <span>in this strange ontological space, are not a sequence of words and punctuation as we might expect. Instead, documents are more like a word census</span><span>; a</span><span>&nbsp;sum totals of the number of times each word occurs in the original, natural document.</span> <span>Choosing exactly what unit of text will come to represent an individual document is a bit of an art form in topic modeling. Text’s natural partitions do not always yield the best results. For example, if you are modeling books, you might treat individual chapters or sections of a chapter as an individual document rather than the entire book. As usual, it is important to understand the nature of the text you are topic modeling to determine the appropriate unit of analysis.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>To briefly recap,</span> <span class="c5">words</span><span>&nbsp;are not words as we typically understand them. Words in topic modeling are</span> <span class="c5">unigrams, bigrams,</span> <span>or</span><span class="c5">&nbsp;ngrams</span><span>&nbsp;that have been</span> <span class="c5">tokenized,</span><span>&nbsp;</span><span class="c5">filtered, stemmed</span><span>&nbsp;and</span><span>&nbsp;</span><span class="c5">counted</span><span>. A collection of word counts, that is, the</span> <span class="c5">term</span><span>&nbsp;</span><span class="c5">frequencies</span><span>, represent individual</span> <span class="c5">documents.</span><span>&nbsp;Collections of documents, a</span> <span class="c5">corpus,</span> <span>are seen from the perspective of the model not as a collection of text files filled with sequences of words, but rather as a</span> <span class="c5">term-document matrix</span><span>.</span>
		</p>
		<p class="c15 c3"></p><a href="#" name="08467060577c8448be27acef583f256f68b8b7ad"></a><a href="#" name="0"></a>
		<table cellpadding="0" cellspacing="0" class="c24">
			<tbody>
				<tr class="c16">
					<td class="c10">
						<p class="c3">
							<span class="c12">Vocabulary</span>
						</p>
					</td>
					<td class="c19 c28">
						<p class="c3">
							<span>Document 1</span>
						</p>
					</td>
					<td class="c26 c28">
						<p class="c3">
							<span>Document 2</span>
						</p>
					</td>
					<td class="c19 c28">
						<p class="c3">
							<span>Document 3</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c10">
						<p class="c3">
							<span>humanities</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>8</span>
						</p>
					</td>
					<td class="c26">
						<p class="c3">
							<span>4</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>0</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c10">
						<p class="c3">
							<span>digital</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>8</span>
						</p>
					</td>
					<td class="c26">
						<p class="c3">
							<span>12</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>4</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c10">
						<p class="c3">
							<span>model</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>0</span>
						</p>
					</td>
					<td class="c26">
						<p class="c3">
							<span>0</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>14</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c10">
						<p class="c3">
							<span>...</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>...</span>
						</p>
					</td>
					<td class="c26">
						<p class="c3">
							<span>...</span>
						</p>
					</td>
					<td class="c19">
						<p class="c3">
							<span>...</span>
						</p>
					</td>
				</tr>
			</tbody>
		</table>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>In the term-document matrix, e</span><span>ach row represents a word token</span><span>&nbsp;resulting</span><span>&nbsp;in one row for every word in the corpus</span><span>. This collection of words is</span><span>&nbsp;called the</span> <span class="c5">vocabulary</span><span>. Each column in the matrix represents a single document, as represented by a set of frequencies of the term in a particular row. Often it is the case the term-document matrix contains a lot of zero entries, that is, there are terms in the vocabulary that only show up in some documents but not others (and visa versa). Such a term-document matrix is considered to be</span> <span class="c5">sparse</span><span>.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>The term-document matrix is a data structure, a computationally tractable (to use McCarty’s term) representation of the texts able to be modeled by a computational process. These pre-processing steps transforms a human readable sequence of words into a long list of word tokens, which are then counted for each individual document and (essentially) recorded in an Excel spreadsheet. Once the texts are represented as a matrix of numbers and all the messy human bits have been eliminated,</span> <span>the fun part</span><span>, topic modeling, can begin!</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Such processing is a boring and, I argue, a taken-for-granted assumption overlooked in many tutorials and introductions to topic modeling. Text pre-processing is an</span> <span class="c5">infrastructural</span><span>&nbsp;process, vitally important, but also completely</span> <span class="c5">ordinary</span><span>&nbsp;within the topic modeling</span> <span class="c5">community of practice</span><span>.</span><sup><a href="#ftnt11" name="ftnt_ref11">[11]</a></sup><span>&nbsp;Members of these spaces have already internalized and</span> <span class="c5">normalized</span><span>&nbsp;many crucial knowledge practices making the process of socializing</span> <span>new members difficult. Such process need to be explicitly articulated, creating opportunities for what Lave and Wenger termed</span> <span class="c5">legitimate peripheral participation</span><span>&nbsp;by new members of the practice. The practices of pre-processing text can, at first glance, seem alien to a humanities scholar versed in close reading, but as I will describe below, even bags-of-words can be used to find interesting patterns within texts.</span>
		</p>
		<h4 class="c4 c3">
			<a name="h.s90pn9o26rbs"></a><span class="c18">OK, now we can talk about topics</span>
		</h4>
		<p class="c3">
			<span>Perhaps the most confusing aspect of topic modeling to a newcomer is the term “topic.” Topic does</span> <span class="c5">not</span><span>&nbsp;mean “a matter dealt with in text, discourse, or conversation” or “a subject” or anything a reasonable person might consider a “topic” if you asked them on the street. A topic, in the domain of language models, means a</span> <span class="c5">probability distribution</span> <span>over a vocabulary of</span><span class="c5">&nbsp;</span><span>words</span><span class="c5">.</span><span>&nbsp;This means, given a list of words, each has a specific value between zero and one (or alternatively, 0% to 100%) associated with that word. The list of values represents an individual topic and different topics will (hopefully) have different values associated with each</span> <span>word</span><span>. &nbsp;</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>One simplistic way to think about topic distributions would be as bags of words containing some varying allotment words. When I reach into the bag and pull out a word the likelihood I will pull out any particular word depends upon the allotment</span><span>&nbsp;of words in the bag</span><span>. However, the exact word you choose is unknown until you actually reach in the bag.</span>
		</p>
		<p class="c15 c3"></p><a href="#" name="fdfb923a50a9877d8b54bd59c35011a9dcdb8198"></a><a href="#" name="1"></a>
		<table cellpadding="0" cellspacing="0" class="c24">
			<tbody>
				<tr class="c16">
					<td class="c7 c14">
						<p class="c3">
							<span>Topic</span>
						</p>
					</td>
					<td class="c23 c14">
						<p class="c15 c17 c3"></p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c7 c14">
						<p class="c3">
							<span>Word</span>
						</p>
					</td>
					<td class="c23 c14">
						<p class="c3 c17">
							<span>Probability</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c7">
						<p class="c3">
							<span class="c9">humanities</span>
						</p>
					</td>
					<td class="c23">
						<p class="c3">
							<span class="c9">0.01</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c7">
						<p class="c3">
							<span class="c9">unigram</span>
						</p>
					</td>
					<td class="c23">
						<p class="c3">
							<span class="c9">0.0004</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c7">
						<p class="c3">
							<span class="c9">digital</span>
						</p>
					</td>
					<td class="c23">
						<p class="c3">
							<span class="c9">0.03</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c7">
						<p class="c3">
							<span class="c9">model</span>
						</p>
					</td>
					<td class="c23">
						<p class="c3">
							<span class="c9">0.02</span>
						</p>
					</td>
				</tr>
				<tr class="c16">
					<td class="c7">
						<p class="c3">
							<span class="c9">...</span>
						</p>
					</td>
					<td class="c23">
						<p class="c3">
							<span class="c9">...</span>
						</p>
					</td>
				</tr>
			</tbody>
		</table>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>For example, in the distribution shown in the table above, I would have a 1% likelihood of selecting the word</span> <span class="c9">humanities</span><span>, a 3% probability of selecting the word</span> <span class="c9">digital</span><span>, a 2% chance of selecting</span> <span class="c9">unigram</span><span class="c9">&nbsp;</span><span>and miniscule (.04%) chance of selecting</span> <span class="c9">model</span><span>. Also important, and not necessarily intuitive, is that each selection of a word is</span> <span class="c5">independent</span><span>&nbsp;so my selections do not affect any subsequent selections,</span> <span class="c5">even of the same word.</span> <span>This would mean if my topic distribution assigned 99% probability to the word</span> <span class="c9">computer</span><span>&nbsp;I will most likely select the word</span> <span class="c9">computer</span><span>&nbsp;every time I draw from the distribution.</span>
		</p>
		<p class="c2">
			<img height="401" src="images/image03.png" width="493">
		</p>
		<p class="c3">
			<span>In this non-artist’s rendition of a topic, the brown squiggles along the bottom represent a vocabulary of words and the grey peaks represent individual word’s</span> <span class="c5">probability density.</span><span>&nbsp;I should note, it is very unlikely you might find a topic</span><span>&nbsp;</span><span>like the one above, with such dramatic peaks and valleys, in the wild. In my (limited) experience the topic distributions are relatively flat with some small clusters of words having a bit more weight than others. The list of</span> <span class="c5">top words</span><span>, words that are “heavy” with more probabilistic mass, are the interesting group of words to examine because</span> <span>they</span><span>&nbsp;are the</span> <span>co-occurring words in</span><span>&nbsp;</span><span>that</span><span>&nbsp;topic distribution.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Now that you (hopefully) understand what words, documents, and topic means from the perspective of a topic model it is time to discuss the generative models themselves.</span>
		</p>
		<h3 class="c25 c3">
			<a name="h.ul1cuqsgyk5v"></a><span>A Brief History of Generative Topic Models</span>
		</h3>
		<p class="c3">
			<span>One of the best ways to understand the assumptions of generative language models is to start with simplistic</span> <span>models</span><span>&nbsp;and then work</span> <span>up</span><span>&nbsp;</span><span>to</span><span>&nbsp;modern topic modeling techniques like LDA. I am drawing heavily here upon Blei et al.</span><sup><a href="#ftnt12" name="ftnt_ref12">[12]</a></sup><span>&nbsp;and section four of the original LDA paper, but instead of contrasting these models with LDA, I want to build up our understanding of each model through the innovations they introduced. Starting with the simple</span> <span class="c5">unigram model</span><span>, to the</span> <span class="c5">mixture of unigrams</span><span>, to</span> <span class="c5">probabilistic latent semantic analysis</span><span>, to</span> <span class="c5">latent dirichlet allocation</span><span>. Each model rests upon a</span><span>&nbsp;</span><span>complex mathematical</span> <span>foundation</span><span>;</span> <span>I am going to gloss over the math and focus more upon intuitive, but not overly simplistic, descriptions of each model’s assumptions. &nbsp;</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>As I discussed above, these are</span> <span class="c5">generative</span> <span>models.</span> <span>Each represents</span> <span>generative process</span><span>&nbsp;</span><span>that repeats on a loop, selecting word tokens from a probabilistic bag-of-words (topics) and generating unique documents from</span> <span>increasing</span><span>ly complex combinations and mixtures of these bags. The models generate words, topics, and documents as I have just</span><span>&nbsp;explained above, not the</span><span>&nbsp;infinitely rich structures of writing and language</span><span>&nbsp;you are reading right not</span><span>.</span><sup><a href="#ftnt13" name="ftnt_ref13">[13]</a></sup><span>&nbsp;With each model I am including a representation in plate notation, a way of visually representing graphical models, and a description of the generative procedure in pseudocode.</span> <span>In the plate notation, a square means a looping, repeating process and a square within a square means nested loops. Circles represent variables, the shaded circles are</span> <span class="c5">observed</span><span>&nbsp;variables–things we have–and the white circles are</span> <span class="c5">latent</span><span>–things we</span> <span class="c5">assume</span><span>&nbsp;are there. Topics are always latent variables, white circles, because in these models assume the existence of topics and make them a set of variables to estimate.</span> <span>In both cases I have attempted to simplify</span><span>&nbsp;</span><span>these representations to make them slightly less intimidating to someone unfamiliar with such forms of notation.</span> <span>I have used english descriptions of variables instead of Greek characters to reduce complexity</span><span>.</span>
		</p>
		<h4 class="c4 c3">
			<a name="h.d3o7l36k1z86"></a><span class="c18">Unigram</span>
		</h4>
		<p class="c3">
			<span>One of the most simplistic language models, although not always considered a “topic model,” is the</span> <span class="c5">unigram language model</span><span>. This model uses a single topic in the entire corpus. &nbsp;Each document in the corpora is composed of some number of words selected from a single topic distribution for the entire corpus.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>The generative process of the unigram model is described in pseudo code below</span><span>:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">For each document in the corpus do the following:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each word in the document do the following:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a word from the word distribution.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Intuitively, the model generates a document by repeatedly selecting words from a single word distribution, i.e. topic. Each word selection is independent from the words selected before and after, which means, given a word distribution where one word is highly</span> <span>likely, that</span><span>&nbsp;word will frequently show up in any generated document. For example, if your word distribution, your bag of words, is about food it</span> <span>might</span><span>&nbsp;</span><span>assign</span><span>&nbsp;</span><span>more weight to the word</span> <span>‘</span><span>pizza.</span><span>’</span><span>&nbsp;When you draw however many words from the distribution you want to</span> <span>‘</span><span>compose</span><span>’</span><span>&nbsp;your document, chances are you will draw several instances of the word</span> <span>‘</span><span>pizza.</span><span>’</span><span>&nbsp;Because I only have one topic distribution, that is, one bag of words, the kinds documents and corpora I can represent with this model are, probabilistically, not very likely. The chances I will be able to generate a document about</span> <span>‘</span><span>automobiles,</span><span>’</span><span>&nbsp;is, probabilistically, less likely if I am given a word distribution weighted in favor of food.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c2">
			<img height="232" src="images/image04.png" width="232">
		</p>
		<p class="c3">
			<span>The image above describes the model using plate</span> <span>notation</span><span>. The outer square represents an iteration over every single document. The inner square represents an iteration over every word for each document. The grey circle in the middle represents the</span> <span class="c5">observed</span><span>&nbsp;variable, in this case the words in each</span> <span>document</span><span>. The shaded</span> <span>circle is the observed word token we select from the word distribution, a.ka. topic (</span><span class="c5">below)</span><span>, and it is encapsulated by two squares meaning it is nested within two loops. The outer square loops over every document in the corpus and the inner square loops over every word in a document.</span>
		</p>
		<p class="c2">
			<img height="241" src="images/image05.png" width="306">
		</p>
		<p class="c3">
			<span>&nbsp;</span>
		</p>
		<p class="c3">
			<span>In the unigram model, the</span><span>&nbsp;bag never changes as we select word after word and compose document after document.</span> <span>Only accommodating a</span><span>&nbsp;single topic dist</span><span>ribution</span><span>&nbsp;limits</span> <span>the unigram model</span><span>’s capacity</span><span>&nbsp;to effectively model the complexity and richness of many human authored corpora. This is not to say the unigram model is not useful, it has been used to great effect in information</span> <span>retrieval</span><span>, but its effectiveness as a topic model is low.</span>
		</p>
		<h4 class="c4 c3">
			<a name="h.o8yhswe97s8i"></a><span class="c18">Mixture of Unigrams</span>
		</h4>
		<p class="c3">
			<span>The</span> <span class="c5">mixture of unigrams</span><span>&nbsp;</span><span>model</span><span>&nbsp;introduces the possibility of multiple</span> <span class="c5">topics,</span> <span>that is, more than one bag from which to draw words when generating documents. The mixture of unigram model introduces a new distribution, a distribution of</span> <span class="c5">topics</span><span>, from which we draw a new distribution of words for each document.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c2">
			<img height="335" src="images/image06.png" width="403">
		</p>
		<p class="c3">
			<span>The generative process of the mixture of Unigrams model is described below:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">For each document in the corpus do the following:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a</span> <span class="c1 c9">distribution of word</span><span class="c1 c9">s(topic) from a distribution of topics.</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each word in the document do the following:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a word from that distribution of words.</span>
		</p>
		<p class="c2">
			<img height="291" src="images/image01.png" width="407">
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>The</span> <span>mixture of unigrams model</span><span>&nbsp;is represented in plate notation above. The mixture of unigrams adds a new</span><span>&nbsp;</span><span class="c5">latent,</span><span>&nbsp;or unobserved, variable</span><span>&nbsp;to the model that</span> <span>represents the topic</span><span>, the word distribution, from which each document will be drawing words. Because</span> <span>the</span><span>&nbsp;latent variable is outside the inner square,</span> <span>&nbsp;there is only</span> <span>one topic per document</span><span>. This is better than</span><span>&nbsp;the unigram model which only allows one topic per corpus. This adds a bit more diversity to the model</span> <span>of the corpus</span><span>, but not necessarily much diversity to any individual document. This means, while the corpus might be about food and books, a single document is about either food or books, but not</span> <span>both</span><span>.</span>
		</p>
		<h4 class="c3 c4">
			<a name="h.tubunkwa0afl"></a><span class="c18">Probabilistic Latent Semantic Analysis</span>
		</h4>
		<p class="c3">
			<span>Probabilistic Latent Semantic Analysis(PLSA) also called &nbsp;Latent Semantic Indexing, introduced in 1999 by Thomas Hofmann, was one of the early and popular topic models. Hofmann’s model introduced several novel innovations over the simplistic techniques I described above. Like the mixture of unigrams model, PLSA models multiple topics or word distributions in the corpus, but, unlike the mixture of unigrams, PLSA allows individual documents to be composed of multiple topics. PLSA does this by sampling a distribution of topics each time we draw a word, instead of each time we create a document. The generative process of the PLSA model is described below:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">For each document in the corpus do the following:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each word in the document do the following:</span>
		</p>
		<p class="c6 c22 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a distribution of words from the distribution of topics.</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a word from that distribution of words.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Notice in the plate notation below how the inner square, the “words in document”</span> <span>iteration, has</span><span>&nbsp;expanded to encompass the latent topic variable.</span> <span>T</span><span>he arrows indicate a dependency</span><span>;</span><span>&nbsp;before a word is drawn from a topic,</span> <span>a</span><span>&nbsp;new topic must be drawn from a distribution. Each document has it’s own unique distribution or mixture of topics. This allows individual documents to be composed of words drawn from multiple topics; a more plausible model of a document’s</span> <span>reality</span><span>.</span>
		</p>
		<p class="c2">
			<img height="313" src="images/image07.png" width="573">
		</p>
		<p class="c3">
			<span>However, as Blei et al. point out, the ways in which the document</span> <span>mixture</span><span>s are created are prone to</span> <span class="c5">overfitting</span><span>, that is, the mode by which an individual document’s topic mixture is established is not robust enough to handle the addition of new documents to the corpus after the model has been generated, or</span> <span class="c5">trained</span><span>&nbsp;in machine learning terms. Overfitting can be a real problem if you are using topic models to work with new documents, for example, using topic models to generate recommendations in a scholarly journal database. If you initially train your PLSA topic model on the articles you have, as you receive new articles the recommendations will get progressively worse unless you retrain using the entire updated corpus. For very large corpora, this can be prohibitively expensive</span> <span>computationally</span><span>.</span>
		</p>
		<p class="c15 c3"></p>
		<h4 class="c4 c3">
			<a name="h.oj2q7mn7k8wt"></a><span class="c18">The Overfitting Problem</span>
		</h4>
		<p class="c3">
			<span>The problem of overfitting marks an interesting distinction between how computer scientists and digital humanists might use topic modeling. One of the benefits of LDA over PLSA is, as I describe below, a robust method for generating a document’s topic mixture. This feature allows a model trained on an existing corpus to identify the topic mixture of new documents without re-training the entire corpus. When, as is trendy in computer science these days, you start talking about “big data,” that is, massive corpora such as the Google Books dataset, training a model becomes computationally expensive.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>However, in the digital humanities, our corpora are often (but not always) meso-scale, or as I like to put it, “bigger than a laptop smaller than a large hadron collider.” Furthermore, it is often the case there will</span> <span class="c5">never</span> <span>be any</span> <span>additional</span><span>&nbsp;documents in our corpora. There is never going to be any</span> <span class="c5">new</span><span>&nbsp;19th century British and American</span> <span>literature</span><span>. &nbsp;I acknowledge this is a grossly simplistic assumption about literary history and the complexities of digitization, but once a historical collection has been fully digitized it should be reasonable not to expect new documents in the corpus.</span> <span>Thus, if the</span> <span>text model you are generating is exclusively for the purposes of exploring a fixed corpus, is overfitting a problem?</span>
		</p>
		<h4 class="c4 c3">
			<a name="h.hszd7eapbh2n"></a><span class="c18">Latent Dirichlet Allocation</span>
		</h4>
		<p class="c3">
			<span>Latent Dirichlet Allocation(LDA) is very similar to PLSA. If you look at the the plate notation LDA there are only two additional, though very important, latent variables added to the model.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c2">
			<img height="330" src="images/image02.png" width="577">
		</p>
		<p class="c3">
			<span>These two</span> <span>corpus</span><span>-</span><span>level</span><span>&nbsp;parameters introduce a Bayesian</span> <span>method</span><span>&nbsp;for sampling the mixture of topics within each document. Essentially this means random sampling but not sampling just any old tea leaves or turtle shells. LDA draws randomly from a parameterized Dirichlet distribution producing, through the magic of mathematics, robust topic mixtures and word distributions able to overcome the overfitting problems of PLSA. Additionally there are fewer parameters to estimate which is important when training the model. The generative process of the LDA model is described below:</span>
		</p>
		<p class="c6 c3 c27">
			<span class="c1 c9">For each document in the corpus do the following steps:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a topic mixture distribution from a Dirichlet distribution.</span>
		</p>
		<p class="c3 c6">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each word in the document do the following steps:</span>
		</p>
		<p class="c6 c3">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a topic from the topic mixture distribution.</span>
		</p>
		<p class="c6 c3 c31">
			<span class="c1 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a word from the word distribution selected above.</span>
		</p>
		<p class="c3">
			<span>LDA describes a generative process whereby, given a Dirichlet</span> <span class="c5">conditioned</span> <span>bag filled with topic-distributions for each document,</span> <span>we</span><span>&nbsp;draw a topic mixture from this bag. Then, we repeatedly draw both a topic and then a word from that topic to generate the words in that document.</span> <span>Voila</span><span>, we have a generative model that represents the process by which a corpora of documents were created. But what about if we already have a corpus of documents?</span>
		</p>
		<h4 class="c4 c3">
			<a name="h.b3aqbki0tq33"></a><span class="c18">Parameter Estimation</span>
		</h4>
		<p class="c3">
			<span>Everything I have described so far</span><span>,</span> <span>about the structure and underlying assumptions of generative topic models</span> <span>is how topic modeling works</span> <span class="c5">in practice</span><span>. When we use topic modeling to</span> <span>model a corpus of text, what are practically trying to do is</span> <span class="c5">estimate</span><span>&nbsp;the parameters of the model as I have described. That is,</span> <span>we are trying to</span> <span>find a</span> <span>model whose parameters</span> <span>have</span> <span>a high likelihood of generating the corpus</span> <span class="c5">if</span> <span>we were to</span> <span>use a generative process to create the corpus we have</span><span>.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>This is a</span> <span class="c5">very</span><span>&nbsp;important distinction between topic modeling as theoretically understood and topic modeling in implementation (practice). The models I have described, in as plain of english as I can muster, are theoretical articulations of a generative process. Given a set of parameters, for LDA this would be word distributions (topics) and a topic mixtures, the process would repeatedly sample these distributions to generate a term-frequency matrix, i.e. a corpus. However, this is not how we use topic modeling in practice. Instead of having the parameters for these various distributions</span> <span class="c5">a priori</span><span>&nbsp;we have, after text pre-processing, a corpus that has been generated by some topic model.</span> <span>The</span><span>&nbsp;goal of topic modeling in practice is to find</span> <span>the model, that is, find the</span> <span>document topic mixtures and word distributions that</span><span>&nbsp;generated th</span><span>e corpus you have.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c15 c3"></p>
		<p class="c2">
			<img height="338" src="images/image00.png" width="491">
		</p>
		<p class="c3">
			<span>The secret to successful topic modeling is</span> <span class="c5">estimating</span><span>&nbsp;the distributions from the set of all possible distributions (a</span><span>n</span> <span class="c5">extremely</span> <span>large space impossible to fully enumerate)</span><span>&nbsp;that best fits the</span> <span>corpus of</span> <span>documents at hand. This</span> <span>process</span><span>, called parameter estimation, is</span><span>&nbsp;</span><span>where much of the mathematical complexity in topic modeling lives. There are many ways to estimate the parameters</span><span>;</span><span>&nbsp;</span><span>the original LDA paper used a process called</span> <span class="c5">variational inference</span><span>&nbsp;and the MALLET toolkit uses a process called</span> <span class="c5">Gibbs Sampling.</span> <span>David Mimno’s talk at the MITH Topic Modeling workshop</span><sup><a href="#ftnt14" name="ftnt_ref14">[14]</a></sup><span>&nbsp;is an excellent discussion on how exactly he uses Gibbs sampling to estimate the parameters of an LDA topic model in the MALLET</span> <span>toolkit</span><span>.</span><sup><a href="#ftnt15" name="ftnt_ref15">[15]</a></sup><span>&nbsp;</span>
		</p>
		<p class="c3 c15"></p>
		<p class="c3">
			<span>My goal here was simply to unpack, in detail, a non-mathematical description of the LDA generative model in hopes that others will better understand how it is exactly that we can use these techniques to explore and understand bodies of text too large to simply read by hand. By understanding these underlying assumptions of generative language models we can first and foremost be better informed about the kinds of claims we make when we use them, but also potentially contribute in making even more robust and pragmatically useful language models for future digital humanists.</span>
		</p>
		<p class="c15 c3"></p>
		<h3 class="c25 c3">
			<a name="h.p8gum61c72uq"></a><span>What Can Topic Modeling Tell Us?</span>
		</h3>
		<p class="c3">
			<span>To ground this discussion, I provide some example output from the MALLET toolkit. Listed below are the top ten words from four of the ten topics I estimated based on a corpus of blog posts from Digital Humanities Now’s Editor Choice selections</span>
		</p>
		<p class="c3 c27">
			<span class="c1 c12 c9">Topic</span>
		</p>
		<p class="c3">
			<span class="c1 c12 c9">0:</span><span class="c1 c9">&nbsp;students education cr learning student free courses class university higher</span>
		</p>
		<p class="c3">
			<span class="c1 c12 c9">1:</span><span class="c1 c9">&nbsp;library access digital content public libraries future google art impact</span>
		</p>
		<p class="c3">
			<span class="c1 c12 c9">2:</span><span class="c1 c9">&nbsp;data visualization information objects mining http heritage open april big</span>
		</p>
		<p class="c3 c31">
			<span class="c1 c12 c9">3:</span><span class="c1 c9">&nbsp;knowledge thinking history historical human point kind understanding place creating</span><span class="c9">&nbsp;</span>
		</p>
		<p class="c3">
			<span>I fed MALLET a set of text files, a number of iterations, and a number of topics. MALLET tokenized my text, removed stop words (the toolkit does not perform stemming for reasons articulated by the author on MALLET mailing list</span><sup><a href="#ftnt16" name="ftnt_ref16">[16]</a></sup><span>), and estimated the word distribution for ten topics and the topic mixtures for each document in the corpus.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>While mainly a science, topic modeling has aspects of an art form. There are several parameters that we must specify before estimating the model. The most significant of these parameters is the number of topics. In the example above, I have selected 10 topics. The number of topics is a subjective selection dependent upon the size and shape of the corpus.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Each document is associated to each topic by some proportion. Just as every topic has a ranked probabilities of words, every document has a ranked probability of topics. Thus, while every document might have some trace of every topic, generally we are only interested in the top one, two or three topics associated with each document. It is fairly common, when analyzing the models, to set some frequency threshold for the document/topic relation (say 10%) so that you attend the topics best represented in a document (or conversely, the top documents in the topic in question).</span>
		</p>
		<p class="c22 c3 c27">
			<span class="c9">Document: file1.txt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">Topic Probability</span>
		</p>
		<p class="c22 c3">
			<span class="c9">3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.3986013986013986&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.12665112665112666&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.11888111888111888&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c3 c22">
			<span class="c9">2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.07459207459207459&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.07381507381507381&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.05439005439005439&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.05128205128205128&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.04895104895104895&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c22 c3">
			<span class="c9">0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.041181041181041184&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
		</p>
		<p class="c3 c13">
			<span class="c9">9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.011655011655011656</span>
		</p>
		<p class="c3">
			<span>In the example above, the topic proportions for file1.txt are ranked from highest to lowest. Topic 3 is the most prominent with a proportion of 39%, followed by topic 4 at 12% and topic 1 with 11%. Topics 0 and 9 are the lowest with 4% and 1% respectively. The document in question is a blog post by Peter Organisciak, a graduate student at the University of Illinois who was actually one of the founders of the “Day of DH” blogging project.</span><sup><a href="#ftnt17" name="ftnt_ref17">[17]</a></sup><span>&nbsp;The post begins with this self-reported summary of it's content:</span>
		</p>
		<p class="c34 c6 c3 c27 c20 c31">
			<span class="c1">"Last month, I gave a presentation about paid crowdsourcing in the humanities at SDH-SEMI. Below are my notes."</span><sup class="c1"><a href="#ftnt18" name="ftnt_ref18">[18]</a></sup>
		</p>
		<p class="c3">
			<span>So the post is about "paid crowdsourcing in the humanities" and according to the topic model, the topic with the highest proportion, topic 3, contains these top words:</span>
		</p>
		<p class="c6 c3 c27 c20 c31 c34">
			<span class="c1 c9">books time people texts make terms research don work things simply sense ways fact change early process read human</span>
		</p>
		<p class="c3">
			<span>Given a very</span><span>&nbsp;</span><span>cursory</span><span>&nbsp;analysis of this unrefined model, I think there is some sense to be made from this topic. Crowdsourcing is all about taking advantage of</span> <span class="c5">people</span><span>’s free</span> <span class="c5">time</span><span>&nbsp;to do certain kinds of</span> <span class="c5">work,</span><span>&nbsp;generally</span> <span class="c5">simple</span><span>&nbsp;tasks, often for the purposes of</span> <span class="c5">research</span><span>. Obviously, to do this analysis justice I would want to go back and see how these terms are used in the original text. Additionally, I would probably want to tweak my model to include more or less topics depending on the dynamics of the corpus.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>There a several implementations of the Latent Dirichlet Allocation available to a researcher interested in topic modeling. LDA-C is perhaps one of the most widely known, it was implemented by David Blei using the C programming language. Perhaps the other most popular implementation of LDA is part of a java toolk</span><span>it, M</span><span>ALLET, maintained by Andrew McCallum at the University of Massachusetts Amherst.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Given that topics are merely lists of words, any topic modeling exercise requires some</span> <span class="c5">interpretive</span><span>&nbsp;effort to discern if the model is a reasonable representation of the corpus and what that representation means. Thus, a close and careful reading of the relationship between topics and documents is necessary to fully understand and contextualize what the words of a topic really</span> <span class="c5">mean</span><span>.</span>
		</p>
		<p class="c15 c3"></p>
		<p class="c3">
			<span>Topic modeling clusters sets of documents according to latent themes and provides a set of keywords associated with that theme. Reading topic models then is an exercise in reading the documents with high proportions for each topic paying special attention to how the set of keywords are used both within and across those high proportion documents. In a sense we might think about reading these documents as editorialized selections, but unfortunately, the editor who put them together has been mysteriously struck with amnesia and all we have are a list of underlined words in each document. With this information we must engage in a semi-hermeneutic exercise of constructing the latent meanings beneath the surface of this scraps of information.</span>
		</p>
		<hr class="c35">
		<h4>Please feel free to comment via the Google Internets <a href="https://docs.google.com/document/d/1ZKX0vsCppB6cjciPldKwG7WWBowMeSFWTC50MjA5fjg/edit?usp=sharing">here</a>.</h4>
		<hr class="c35">
		<div>
			<p class="c3">
				<a href="#ftnt_ref1" name="ftnt1">[1]</a><span>&nbsp;</span><span class="c1">Ted Underwood points out that while word counts are simplistic, they are still extremely powerful. The full richness of words themselves, he argues, are still not a fully utilized feature for machine learning algorithms. In the comments Ryan Shaw points to another blog post by Brendan O’Conner which succinctly</span><span class="c1">&nbsp;and brilliantly observes:</span><span class="c1">&nbsp;“Words are already a massive dimension reduction of the space of human experiences.”</span> <span>&nbsp;</span><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://tedunderwood.com/2013/02/20/wordcounts-are-amazing/">http://tedunderwood.com/2013/02/20/wordcounts-are-amazing/</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3">
				<a href="#ftnt_ref2" name="ftnt2">[2]</a><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://www.matthewjockers.net/macroanalysisbook/lda/">http://www.matthewjockers.net/macroanalysisbook/lda/</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3">
				<a href="#ftnt_ref3" name="ftnt3">[3]</a><span class="c1">&nbsp;Blei, David M, Andrew Y Ng, and Michael I Jordan. "Latent dirichlet allocation."</span> <span class="c0">the Journal of machine Learning research</span><span class="c1">&nbsp;3 (2003): 993-1022.</span>
			</p>
		</div>
		<div>
			<p class="c3">
				<a href="#ftnt_ref4" name="ftnt4">[4]</a><span class="c1">&nbsp;</span><span class="c1 c11"><a class="c8" href="http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/">http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref5" name="ftnt5">[5]</a><span class="c1">&nbsp;McCarty, Willard. "Modeling: a study in words and meanings."</span> <span class="c0">A companion to digital humanities</span><span class="c1">&nbsp;(2004): 254-270.</span> <span class="c11 c1"><a class="c8" href="http://nora.lis.uiuc.edu:3030/companion/view?docId=blackwell/9781405103213/9781405103213.xml&amp;chunk.id=ss1-3-7&amp;toc.depth=1&amp;toc.id=ss1-3-7&amp;brand=9781405103213_brand">http://nora.lis.uiuc.edu:3030/companion/view?docId=blackwell/9781405103213/9781405103213.xml&amp;chunk.id=ss1-3-7&amp;toc.depth=1&amp;toc.id=ss1-3-7&amp;brand=9781405103213_brand</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3">
				<a href="#ftnt_ref6" name="ftnt6">[6]</a><span class="c1">&nbsp;Hello World! Processing.</span> <span class="c11 c1"><a class="c8" href="http://vimeo.com/60731302#t=1800">http://vimeo.com/60731302#t=1800</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref7" name="ftnt7">[7]</a><span class="c1">&nbsp;Polyani, Michael. "The tacit dimension." (1966).</span>
			</p>
		</div>
		<div>
			<p class="c3">
				<a href="#ftnt_ref8" name="ftnt8">[8]</a><span class="c1">&nbsp;Emphasis in the original</span><span class="c1">.</span> <span class="c1">McCarty, Willard. "Modeling: a study in words and meanings." A companion to digital humanities (2004): 254-270</span><span class="c30">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://nora.lis.uiuc.edu:3030/companion/view?docId=blackwell/9781405103213/9781405103213.xml&amp;chunk.id=ss1-3-7&amp;toc.depth=1&amp;toc.id=ss1-3-7&amp;brand=9781405103213_brand">http://nora.lis.uiuc.edu:3030/companion/view?docId=blackwell/9781405103213/9781405103213.xml&amp;chunk.id=ss1-3-7&amp;toc.depth=1&amp;toc.id=ss1-3-7&amp;brand=9781405103213_brand</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref9" name="ftnt9">[9]</a><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://en.wikipedia.org/wiki/Tf-idf">http://en.wikipedia.org/wiki/Tf-idf</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3">
				<a href="#ftnt_ref10" name="ftnt10">[10]</a><span class="c1">&nbsp;Porter, Martin F. "An algorithm for suffix stripping." 14.3 (1980): 130-137.</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref11" name="ftnt11">[11]</a><span class="c1">&nbsp;Lave, Jean. "Situating learning in communities of practice."</span> <span class="c0">Perspectives on socially shared cognition</span><span class="c1">&nbsp;63 (1991): 82.</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref12" name="ftnt12">[12]</a><span class="c1">&nbsp;Blei, David M, Andrew Y Ng, and Michael I Jordan. "Latent dirichlet allocation."</span> <span class="c0">the Journal of machine Learning research</span><span class="c1">&nbsp;3 (2003): 993-1022.</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref13" name="ftnt13">[13]</a><span class="c1">&nbsp;This of course assumes my writing is “infinitely rich” and “complex.”</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref14" name="ftnt14">[14]</a><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://journalofdigitalhumanities.org/2-1/the-details-by-david-mimno/">http://journalofdigitalhumanities.org/2-1/the-details-by-david-mimno/</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref15" name="ftnt15">[15]</a><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://mallet.cs.umass.edu/topics.php">http://mallet.cs.umass.edu/topics.php</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref16" name="ftnt16">[16]</a><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://comments.gmane.org/gmane.comp.ai.mallet.devel/1724">http://comments.gmane.org/gmane.comp.ai.mallet.devel/1724</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref17" name="ftnt17">[17]</a><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://tapor.ualberta.ca/taporwiki/index.php/Day_in_the_Life_of_the_Digital_Humanities">http://tapor.ualberta.ca/taporwiki/index.php/Day_in_the_Life_of_the_Digital_Humanities</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
		<div>
			<p class="c3 c20">
				<a href="#ftnt_ref18" name="ftnt18">[18]</a><span class="c1">&nbsp;</span><span class="c11 c1"><a class="c8" href="http://www.porganized.com/blog/a-modest-payment-for-a-modern-proposal">http://www.porganized.com/blog/a-modest-payment-for-a-modern-proposal</a></span><span class="c1">&nbsp;</span>
			</p>
		</div>
	</body>
</html>